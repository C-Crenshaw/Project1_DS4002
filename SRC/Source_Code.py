# -*- coding: utf-8 -*-
"""DS 4002 Project 1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YPt-yQZR_OffM-WwLsKOsho0I5PVS_6A

#Exploring the Relationship Between Connotation of News Articles and Company Revenue Using Sentiment Analysis

##All Imports
"""

#Standard imports
import pandas as pd
import numpy as np
import random
import plotly.express as px

#Natural Language Toolkit (NLTK) import and pre-trained model and other resources download
import nltk
nltk.download('all')
#45s runtime

#Other NLKT imports
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

#ML imports
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler

"""##Data Cleaning"""

p = 0.01  # Take 1% of the population to generate a sample
#Original and raw dataset is from https://www.kaggle.com/datasets/aryansingh0909/nyt-articles-21m-2000-present
nyt = pd.read_csv(r"content/nyt-metadata.csv", header=0, skiprows=lambda i: i>0 and random.random() > p, low_memory=False)
nyt = nyt.drop(columns=(['web_url', 'uri', '_id', 'byline', 'subsection_name', 'document_type','multimedia', 'source', 'snippet', 'keywords']))

# Clean headline column and create new column
headline = []
start = "'main': "
end = ", 'kicker': "
i = 0
for value in nyt["headline"]:
    idx1 = value.index(start)
    idx2 = value.index(end)
    headline = np.append(headline,value[idx1 + len(start) + 1: idx2-1])

# Create new column
nyt['headlines'] = headline

# Drop old headline column
nyt = nyt.drop(columns=(['headline']))

# Rearrange column order (personal preference)
new_cols = ['headlines', 'abstract', 'lead_paragraph', 'print_section', 'print_page', 'pub_date', 'news_desk', 'section_name', 'type_of_material', 'word_count']
nyt=nyt.reindex(columns=new_cols)

# Export new dataset
nyt.to_csv('nyt-metadata-SAMPLE.csv', index=False)

#Read in sample data
nyt = pd.read_excel(r"/content/nyt-metadata-SAMPLE.xlsx")
#15s runtime

print(nyt.head())

#Select only relevant columns (drop blank trailing columns)
nyt = nyt.iloc[:,0:10]

#Convert headlines column to strings
nyt['headlines'] = nyt['headlines'].astype(str)

#Text preprocessing function
def textPreprocess(string):
  #Tokenizing text
  tokens = word_tokenize(string.lower())

  #Removing stop words
  filteredTokens = [token for token in tokens if token not in stopwords.words("english")]

  #Lemmatizing tokens
  lemmatizer = WordNetLemmatizer()
  lemmatizedTokens = [lemmatizer.lemmatize(token) for token in filteredTokens]

  #Joining tokens back together
  processedString = ' '.join(lemmatizedTokens)

  return processedString

#Initializing NLTK sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

#Sentiment fetching function
def sentimentFetch(string):
  scores = analyzer.polarity_scores(string)
  sentiment = scores["compound"]
  return sentiment

#Process headlines and create new sentiment column
nyt["compound_sentiment"] = nyt["headlines"].apply(textPreprocess).apply(sentimentFetch)
#26s runtime

#Positive, neutral, or negative sentiment definer function
def compoundToDirection(score):
  if score >= 0.05:
    return 1
  elif score <= -0.05:
    return -1
  return 0

#Create sentiment direction column
nyt["sentiment_direction"] = nyt["compound_sentiment"].apply(compoundToDirection)

#Extract only publication year of article and create column
nyt["pub_year"] = nyt["pub_date"].str[:4]
nyt['pub_year'] = nyt['pub_year'].astype(str)

print(nyt.head())

# Export sentiment dataset
nyt.to_csv("nyt-sentiment.csv",index=False)

"""###Sentiment Dataset Cleaning"""

#Read in sentiment data
nyt_s = pd.read_csv("/content/nyt-sentiment.csv")

#Convert pub_year column to pandas timestamp
nyt_s['pub_year'] = pd.to_datetime(nyt_s['pub_year'], errors='coerce')
#Drop any NaT values resulting from coerced conversion
nyt_s = nyt_s.dropna(axis=0,subset=['pub_year'])

nyt_s.head()

"""###Revenue Dataset Cleaning"""

#Read in revenue data
nyt_r = pd.read_excel("/content/New York Times Revenue.xlsx")
#Override Year column to only inlcude year
nyt_r["Year"] = nyt_r["Year"].dt.year

nyt_r.head()

"""##ML Dataset Creation"""

#Groupby year and find average compount sentiment and sentiment direction -> strore into dataframe and join by year
nyt_ml = nyt_s.groupby(nyt_s["pub_year"].dt.year)['compound_sentiment'].mean().to_frame().join(nyt_s.groupby(nyt_s["pub_year"].dt.year)['sentiment_direction'].mean().to_frame(), on="pub_year")

print(nyt_ml.head())

#Reset index so that Year is it's own column
nyt_ml = nyt_ml.reset_index()
nyt_ml = nyt_ml.rename(columns={"pub_year":"Year"})

#Merge with revenue dataset on Year column
nyt_ml = nyt_ml.merge(nyt_r[["Year","Revenue_Adjusted (in 2022 dollars)"]], on="Year")
#Rename adjusted revenue column
nyt_ml = nyt_ml.rename(columns={"Revenue_Adjusted (in 2022 dollars)":"Adjusted 2022 Revenue in Billions USD"})



nyt_ml.head()

"""##Data Visualization"""

#Histogram of Top Frequency Counts of Article Sections
fig = px.histogram(nyt, x='section_name',
                   labels={
                     "count": "Count",
                     "section_name": "Section Name",
                     },
                   title="Top Frequency Counts of Article Sections")
fig = fig.update_xaxes(categoryorder='total descending')
fig.show()

#Scatterplot of Revenue vs Compound Sentiment
fig2 = px.scatter(nyt_ml, x="compound_sentiment", y="Adjusted 2022 Revenue in Billions USD",
                 labels={
                     "compound_sentiment": "Compound Sentiment",
                     "Adjusted 2022 Revenue in Billions USD": "Adjusted 2022 Revenue (Billions USD)",
                 },
                title="Adjusted 2022 Revenue vs Compound Sentiment", hover_name = nyt_ml["Year"])
fig2.show()

#Scatterplot of Revenue vs Sentiment Direction
fig3 = px.scatter(nyt_ml, x="sentiment_direction", y="Adjusted 2022 Revenue in Billions USD",
                 labels={
                     "sentiment_direction": "Sentiment Direction",
                     "Adjusted 2022 Revenue in Billions USD": "Adjusted 2022 Revenue (Billions USD)",
                 },
                title="Adjusted 2022 Revenue vs Sentiment Direction", hover_name = nyt_ml["Year"])
fig3.show()

"""##Machine Learning"""

#Last ML data cleaning

#Keep columns relavent for ML
nyt_ml = nyt_ml.drop(columns=["Year"])

#Scale data
nyt_ml[["compound_sentiment","sentiment_direction"]] = MinMaxScaler().fit_transform(nyt_ml[["compound_sentiment","sentiment_direction"]])

#Divide into X & y
X = nyt_ml.drop(columns=["Adjusted 2022 Revenue in Billions USD"])
y = nyt_ml["Adjusted 2022 Revenue in Billions USD"]

#Find averages of MSE, RMSE, and beta coefficients for 1000 models - 6s runtime
mse_avg = 0
rmse_avg = 0
beta1_arr = []
beta2_arr = []
intercept_avg = 0
for i in range(1000):
  #Split data
  X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)
  #Create and train model
  mult_reg = LinearRegression()
  mult_reg.fit(X_train,y_train)

  #Generate Predictions
  predicted = mult_reg.predict(X_test)
  actual = np.array(y_test)

  #MSE & RMSE
  mse = mean_squared_error(predicted,actual)
  mse_avg += mse
  rmse = mean_squared_error(predicted,actual,squared=False)
  rmse_avg += rmse

  #Beta Coefficients
  coef = mult_reg.coef_
  beta1_arr.append(coef[0])
  beta2_arr.append(coef[1])
  intercept = mult_reg.intercept_
  intercept_avg += intercept

mse_avg /= 1000
rmse_avg /= 1000
print("Average MSE is", round(mse_avg,5), "and Average RMSE is", round(rmse_avg,5))

#Average multiple linear regression equation
beta1_avg = np.mean(beta1_arr)
beta2_avg = np.mean(beta2_arr)
intercept_avg /= 1000

print("Adjusted 2022 Revenue in Billions USD =", round(beta1_avg,3),"*compound_sentiment +", round(beta2_avg,3),"*sentiment_direction +", round(intercept_avg,3))